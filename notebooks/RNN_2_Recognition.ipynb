{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import isdir, join\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import multiprocessing as mp\n",
    "# Math\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import pyaudio\n",
    "import wave\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "seq_length = 9 #layer\n",
    "# train Parameters\n",
    "X_dim = 442 #n_dim #X_hot.shape[2]\n",
    "\n",
    "#seq_length = 7 #X_hot.shape[1]\n",
    "output_dim = 1 #n_classes #Y_hot.shape[1]\n",
    "\n",
    "hidden_dim = 2\n",
    "learning_rate = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, X_dim], name = 'X')\n",
    "Y = tf.placeholder(tf.float32, [None, output_dim], name = 'Y')\n",
    "\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "    num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "\n",
    "Y_pred = tf.contrib.layers.fully_connected(\n",
    "    outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "#loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Y_pred, labels = Y))\n",
    "\n",
    "# optimizer\n",
    "lr = tf.placeholder(tf.float32,shape=(), name='learning_rate')\n",
    "train = tf.train.AdamOptimizer(lr).minimize(loss) #AdamOptimizer\n",
    "\n",
    "# RMSE\n",
    "targets = tf.placeholder(tf.float32, [None, output_dim] , name = 'targets')\n",
    "predictions = tf.placeholder(tf.float32, [None, output_dim] , name = 'predictions')\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_specgram(audio, sample_rate, window_size=20,\n",
    "                 step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return freqs, times, np.log(spec.T.astype(np.float32) + eps)\n",
    "def showFreqTime(combine):\n",
    "    for sample, filename, _ in combine:\n",
    "        print(sample)\n",
    "        freqs, times, spectrogram = log_specgram(sample, SR)\n",
    "        fig = plt.figure(figsize=(14, 10))\n",
    "        ax1 = fig.add_subplot(211)\n",
    "        ax1.set_title('Raw wave of ' + filename)\n",
    "        ax1.set_ylabel('Amplitude')\n",
    "        ax1.plot(np.linspace(0, len(sample)/SR, len(sample)), sample)\n",
    "\n",
    "        print(freqs.min(), freqs.max())\n",
    "        print(times.min(), times.max())\n",
    "        ax2 = fig.add_subplot(212)\n",
    "        ax2.imshow(spectrogram.T, aspect='auto', origin='lower', \n",
    "               extent=[times.min(), times.max(), freqs.min(), freqs.max()])\n",
    "        \n",
    "        ax2.set_yticks(freqs[::16])\n",
    "        ax2.set_xticks(times[::16])\n",
    "        ax2.set_title('Spectrogram of ' + filename)\n",
    "        ax2.set_ylabel('Freqs in Hz')\n",
    "        ax2.set_xlabel('Seconds')\n",
    "\n",
    "def printDescription(y_true, y_pred):\n",
    "    p,r,f,s = precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "    print(\"F-Score:\", round(f,3))\n",
    "    print(\"Accuracy: \", accuracy_score(y_true, y_pred))\n",
    "\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "def makeHot(dataX,seq_length):\n",
    "    X_hot_list= []\n",
    "    for i in range(0, dataX.shape[0] - seq_length+1):\n",
    "        _x = dataX[i:i + seq_length]\n",
    "        X_hot_list.append(_x)\n",
    "    X_hot = np.array(X_hot_list[:])\n",
    "    return X_hot\n",
    "def getStream():\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=AUDIO_FORMAT, channels=1, rate=SAMPLE_RATE,\n",
    "            input=True, frames_per_buffer=CHUNK_SIZE)\n",
    "    wf = wave.open(WAVE_FILENAME, 'wb')\n",
    "    wf.setnchannels(1)\n",
    "    wf.setsampwidth(p.get_sample_size(AUDIO_FORMAT))\n",
    "    wf.setframerate(SR)\n",
    "    \n",
    "    t1 = time.time()\n",
    "    # grab audio and timestamp\n",
    "    #data = \n",
    "    #frames.append(data)\n",
    "    audio = np.fromstring(stream.read(CHUNK_SIZE), np.int16)\n",
    "    # write to the audio file\n",
    "    wf.writeframes(b''.join(audio))\n",
    "    t2 = time.time()\n",
    "    print(\"time: %.4f \\t\"%(t2-t1),end='')\n",
    "    \n",
    "    \n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "\n",
    "    p.terminate()\n",
    "    return b''.join(audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/RNN/my_RNN_model_S9_40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jh\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.6736 \t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jh\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:61: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bytes' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-8a1cf9b5bce8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m###Preprocess\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mfreqs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspectrogram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_specgram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mshowFreqTime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maudio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSR\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-146f5415c2c4>\u001b[0m in \u001b[0;36mlog_specgram\u001b[1;34m(audio, sample_rate, window_size, step_size, eps)\u001b[0m\n\u001b[0;32m      8\u001b[0m                                     \u001b[0mnperseg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnperseg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                                     \u001b[0mnoverlap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnoverlap\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                                     detrend=False)\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfreqs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mshowFreqTime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jh\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\scipy\\signal\\spectral.py\u001b[0m in \u001b[0;36mspectrogram\u001b[1;34m(x, fs, window, nperseg, noverlap, nfft, detrend, return_onesided, scaling, axis, mode)\u001b[0m\n\u001b[0;32m    689\u001b[0m     \u001b[1;31m# need to set default for nperseg before setting default for noverlap below\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m     window, nperseg = _triage_segments(window, nperseg,\n\u001b[1;32m--> 691\u001b[1;33m                                        input_length=x.shape[axis])\n\u001b[0m\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m     \u001b[1;31m# Less overlap than welch, so samples are more statisically independent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'bytes' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "\n",
    "model_path_f = '../models/RNN/'\n",
    "filename = 'my_RNN_model_S9_40'\n",
    "\n",
    "#init \n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, model_path_f+filename)\n",
    "\n",
    "mean = np.load('../data/Xy/RNN_mean.npy')\n",
    "std = np.load('../data/Xy/RNN_std.npy')\n",
    "\n",
    "\n",
    "import time\n",
    "CHUNK_SIZE = 8192*5*1\n",
    "AUDIO_FORMAT = pyaudio.paInt16\n",
    "SR = 44100\n",
    "SAMPLE_RATE = SR\n",
    "WAVE_FILENAME = '../data/output1.wav'\n",
    "####\n",
    "justone = True\n",
    "while(justone):\n",
    "    justone = False\n",
    "    ###get Data\n",
    "    audio = getStream()\n",
    "    \n",
    "    ###get Data\n",
    "    #filename1 = '../data/output1.wav'\n",
    "    #audio, sample_rate = librosa.load(filename1)\n",
    "    \n",
    "    ###Preprocess\n",
    "    freqs, times, spectrogram = log_specgram(audio, SR)  \n",
    "    showFreqTime([[audio, filename1, SR]])\n",
    "    \n",
    "    ###Nomalization\n",
    "    spectrogram = (spectrogram - mean) / std\n",
    "    \n",
    "    ###stacked data\n",
    "    X_hot = makeHot(spectrogram,seq_length)\n",
    "    \n",
    "    ###recognization\n",
    "    y_pred = sess.run(Y_pred,feed_dict={X: X_hot})\n",
    "    print(y_pred[:10])\n",
    "    \n",
    "    y_pred[y_pred<0.5] = 0\n",
    "    y_pred[y_pred>=0.5] = 1\n",
    "    \n",
    "    printDescription(np.ones(shape=[y_pred.shape[0]]), y_pred)\n",
    "    \n",
    "    #result\n",
    "    result = y_pred[-1]\n",
    "    if result == 1:\n",
    "        print('The sound is Drone')\n",
    "    else :\n",
    "        print('The sound isn\\'t Drone')\n",
    "sess.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
