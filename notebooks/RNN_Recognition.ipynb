{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import isdir, join\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import multiprocessing as mp\n",
    "# Math\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import pyaudio\n",
    "import wave\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "seq_length = 9 #layer\n",
    "# train Parameters\n",
    "X_dim = 442 #n_dim #X_hot.shape[2]\n",
    "\n",
    "#seq_length = 7 #X_hot.shape[1]\n",
    "output_dim = 1 #n_classes #Y_hot.shape[1]\n",
    "\n",
    "hidden_dim = 2\n",
    "learning_rate = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, X_dim], name = 'X')\n",
    "Y = tf.placeholder(tf.float32, [None, output_dim], name = 'Y')\n",
    "\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "    num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "\n",
    "Y_pred = tf.contrib.layers.fully_connected(\n",
    "    outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "#loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Y_pred, labels = Y))\n",
    "\n",
    "# optimizer\n",
    "lr = tf.placeholder(tf.float32,shape=(), name='learning_rate')\n",
    "train = tf.train.AdamOptimizer(lr).minimize(loss) #AdamOptimizer\n",
    "\n",
    "# RMSE\n",
    "targets = tf.placeholder(tf.float32, [None, output_dim] , name = 'targets')\n",
    "predictions = tf.placeholder(tf.float32, [None, output_dim] , name = 'predictions')\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_specgram(audio, sample_rate, window_size=20,\n",
    "                 step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return freqs, times, np.log(spec.T.astype(np.float32) + eps)\n",
    "def showFreqTime(combine):\n",
    "    for sample, filename, _ in combine:\n",
    "        freqs, times, spectrogram = log_specgram(sample, SR)\n",
    "        fig = plt.figure(figsize=(14, 10))\n",
    "        ax1 = fig.add_subplot(211)\n",
    "        ax1.set_title('Raw wave of ' + filename)\n",
    "        ax1.set_ylabel('Amplitude')\n",
    "        ax1.plot(np.linspace(0, len(sample)/SR, len(sample)), sample)\n",
    "\n",
    "        ax2 = fig.add_subplot(212)\n",
    "        ax2.imshow(spectrogram.T, aspect='auto', origin='lower', \n",
    "               extent=[times.min(), times.max(), freqs.min(), freqs.max()])\n",
    "        ax2.set_yticks(freqs[::16])\n",
    "        ax2.set_xticks(times[::16])\n",
    "        ax2.set_title('Spectrogram of ' + filename)\n",
    "        ax2.set_ylabel('Freqs in Hz')\n",
    "        ax2.set_xlabel('Seconds')\n",
    "def getStream():\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=AUDIO_FORMAT, channels=1, rate=SAMPLE_RATE,\n",
    "            input=True, frames_per_buffer=CHUNK_SIZE)\n",
    "    wf = wave.open(WAVE_FILENAME, 'wb')\n",
    "    wf.setnchannels(1)\n",
    "    wf.setsampwidth(p.get_sample_size(AUDIO_FORMAT))\n",
    "    wf.setframerate(SR)\n",
    "    \n",
    "    t1 = time.time()\n",
    "    # grab audio and timestamp\n",
    "    #data = \n",
    "    #frames.append(data)\n",
    "    audio = np.fromstring(stream.read(CHUNK_SIZE), np.int16)\n",
    "    # write to the audio file\n",
    "    wf.writeframes(b''.join(audio))\n",
    "    audio = audio.astype(float)\n",
    "    t2 = time.time()\n",
    "    print(\"time: %.4f \\t\"%(t2-t1),end='')\n",
    "    \n",
    "    \n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "\n",
    "    p.terminate()\n",
    "    return audio\n",
    "def printDescription(y_true, y_pred):\n",
    "    p,r,f,s = precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "    print(\"F-Score:\", round(f,3))\n",
    "    print(\"Accuracy: \", accuracy_score(y_true, y_pred))\n",
    "\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "def makeHot(dataX,seq_length):\n",
    "    X_hot_list= []\n",
    "    for i in range(0, dataX.shape[0] - seq_length+1):\n",
    "        _x = dataX[i:i + seq_length]\n",
    "        X_hot_list.append(_x)\n",
    "    X_hot = np.array(X_hot_list[:])\n",
    "    return X_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/RNN/my_RNN_model_S9_40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jh\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n",
      "c:\\users\\jh\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:42: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0.7671 \tTHe sound isn't Drone\n",
      "time: 0.7631 \tTHe sound isn't Drone\n",
      "time: 0.7658 \tTHe sound isn't Drone\n",
      "time: 0.7538 \tTHe sound isn't Drone\n",
      "time: 0.7633 \tTHe sound isn't Drone\n",
      "time: 0.7628 \tTHe sound isn't Drone\n",
      "time: 0.7644 \tTHe sound isn't Drone\n",
      "time: 0.7630 \tTHe sound isn't Drone\n",
      "time: 0.7628 \tTHe sound isn't Drone\n",
      "time: 0.7634 \tTHe sound isn't Drone\n",
      "time: 0.7622 \tTHe sound isn't Drone\n",
      "time: 0.7631 \tTHe sound isn't Drone\n",
      "time: 0.7628 \tTHe sound isn't Drone\n",
      "time: 0.7641 \tTHe sound isn't Drone\n",
      "time: 0.7662 \tTHe sound isn't Drone\n",
      "time: 0.7627 \tTHe sound isn't Drone\n",
      "time: 0.7633 \tTHe sound isn't Drone\n",
      "time: 0.7557 \tTHe sound isn't Drone\n",
      "time: 0.7630 \tTHe sound isn't Drone\n",
      "time: 0.7586 \tTHe sound isn't Drone\n",
      "time: 0.7647 \tTHe sound isn't Drone\n",
      "time: 0.7638 \tTHe sound isn't Drone\n",
      "time: 0.7643 \tTHe sound isn't Drone\n",
      "time: 0.7620 \tTHe sound isn't Drone\n",
      "time: 0.7652 \tTHe sound isn't Drone\n",
      "time: 0.7635 \tTHe sound isn't Drone\n",
      "time: 0.7632 \tTHe sound isn't Drone\n",
      "time: 0.7538 \tTHe sound isn't Drone\n",
      "time: 0.7526 \tTHe sound isn't Drone\n",
      "time: 0.7516 \tTHe sound isn't Drone\n",
      "time: 0.7506 \tTHe sound isn't Drone\n",
      "time: 0.7516 \tTHe sound isn't Drone\n",
      "time: 0.7578 \tTHe sound isn't Drone\n",
      "time: 0.7544 \tTHe sound isn't Drone\n",
      "time: 0.7499 \tTHe sound isn't Drone\n",
      "time: 0.7566 \tTHe sound isn't Drone\n",
      "time: 0.7556 \tTHe sound isn't Drone\n",
      "time: 0.7531 \tTHe sound isn't Drone\n",
      "time: 0.7497 \tTHe sound isn't Drone\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-f6d570d55151>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjustone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mjustone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0maudio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetStream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m#get Data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-612a8ee43fe3>\u001b[0m in \u001b[0;36mgetStream\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m#data =\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;31m#frames.append(data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0maudio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCHUNK_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[1;31m# write to the audio file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mwf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriteframes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jh\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pyaudio.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    606\u001b[0m                           paCanNotReadFromAnOutputOnlyStream)\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexception_on_overflow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_read_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_path_f = '../models/RNN/'\n",
    "filename = 'my_RNN_model_S9_40'\n",
    "\n",
    "#init \n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, model_path_f+filename)\n",
    "\n",
    "mean = np.load('../data/Xy/RNN_mean.npy')\n",
    "std = np.load('../data/Xy/RNN_std.npy')\n",
    "\n",
    "\n",
    "import time\n",
    "CHUNK_SIZE = 8192*4\n",
    "AUDIO_FORMAT = pyaudio.paInt16\n",
    "SR = 44100\n",
    "SAMPLE_RATE = SR\n",
    "WAVE_FILENAME = '../data/output.wav'\n",
    "####\n",
    "justone = True\n",
    "while(justone):\n",
    "    justone = True\n",
    "    audio = getStream()\n",
    "    \n",
    "    #get Data\n",
    "    freqs, times, spectrogram = log_specgram(audio, sample_rate)  \n",
    "    #showFreqTime([[sample, filename1, SR]])  \n",
    "\n",
    "    #Nomalization\n",
    "    spectrogram = (spectrogram - mean) / std\n",
    "    \n",
    "    #stacked data\n",
    "    X_hot = makeHot(spectrogram,seq_length)\n",
    "    \n",
    "    #recognization\n",
    "    y_pred = sess.run(Y_pred,feed_dict={X: X_hot})\n",
    "    #print(y_pred[:])\n",
    "    \n",
    "    y_pred[y_pred<0.5] = 0\n",
    "    y_pred[y_pred>=0.5] = 1\n",
    "    \n",
    "    #printDescription(np.zeros(shape=[y_pred.shape[0]]), y_pred)\n",
    "    \n",
    "    #result\n",
    "    result = y_pred[-1]\n",
    "    if result == 1:\n",
    "        print('The sound is Drone')\n",
    "    else :\n",
    "        print('THe sound isn\\'t Drone')\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
